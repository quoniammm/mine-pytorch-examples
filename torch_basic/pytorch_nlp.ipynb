{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f21041b7b10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import jieba\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 1.pytorch basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1,  0],\n",
       "       [ 0,  0,  0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float64_a = np.array(np.random.randn(2, 3), dtype=np.int)\n",
    "float64_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(float64_a[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1 = torch.from_numpy(float64_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.LongTensor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "float64_b = [\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test2 = torch.Tensor(float64_b).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.LongTensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-2  1\n",
       " 0 -2\n",
       "[torch.ShortTensor of size 2x2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 2).type(torch.ShortTensor)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-2\n",
       " 1\n",
       " 0\n",
       "-2\n",
       "[torch.ShortTensor of size 4x1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(4, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 2.Computation Graphs and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_c = torch.Tensor([1., 2., 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(type(test_c[2]))\n",
    "print(test_c[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([1., 2., 3]), requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = Variable(torch.Tensor([4., 5., 6]), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 5\n",
       " 7\n",
       " 9\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.function.AddBackward at 0x7f20ae9854f8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 21\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = z.sum()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.function.SumBackward at 0x7f20ae9855e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 3.Deep learning building blocks: \n",
    "###  Affine maps, non-linearities and objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.4767 -0.6123\n",
       "-0.6477 -0.3223\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Affine maps\n",
    "lin = nn.Linear(5, 2)\n",
    "X = Variable(torch.rand(2, 5))\n",
    "y = lin(X)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3968  0.9355  0.5388  0.8463  0.4192\n",
       " 0.3133  0.6852  0.5245  0.2045  0.4435\n",
       "[torch.FloatTensor of size 2x5]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### non-linearities\n",
    "### BY ACTIVATIONS\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3968  0.9355  0.5388  0.8463  0.4192\n",
       " 0.3133  0.6852  0.5245  0.2045  0.4435\n",
       "[torch.FloatTensor of size 2x5]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.relu(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3968  0.9355  0.5388  0.8463  0.4192\n",
       " 0.3133  0.6852  0.5245  0.2045  0.4435\n",
       "[torch.FloatTensor of size 2x5]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.elu(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### softmax and probabilities\n",
    "X = Variable(torch.randn(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2321  0.0836  0.4662  0.1704  0.0477\n",
       "[torch.FloatTensor of size 1x5]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = F.softmax(X)\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.717 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'love': 1, 'you': 2, 'very': 3, 'much': 4, '我': 5, '非常': 6, '爱': 7, '你': 8, 'Do': 9, 'her': 10, '喜欢': 11, '她': 12, '吗': 13, 'H': 14, 'e': 15, ' ': 16, 'i': 17, 's': 18, 'a': 19, 'g': 20, 'o': 21, 'd': 22, 'p': 23, 'r': 24, 'n': 25, '他': 26, '是': 27, '一个': 28, '好人': 29}\n"
     ]
    }
   ],
   "source": [
    "### bag of words\n",
    "data = [\n",
    "    (\"I love you very much\".split(), \"ENGLISH\"),\n",
    "    (list(jieba.cut(\"我非常爱你\")), \"CHINESE\"),\n",
    "    (\"Do you love her\".split(), \"ENGLISH\"),\n",
    "    (list(jieba.cut(\"你喜欢她吗\")), \"CHINESE\")\n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    (\"He is a good person\", \"ENGLISH\"),\n",
    "    (list(jieba.cut(\"他是一个好人\")), \"CHINESE\")\n",
    "]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(vocab_size, num_labels),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "        #self.linear = nn.Linear(vocab_size, num_labels)\n",
    "    \n",
    "    def forward(self, bow_vec):\n",
    "        #bow_vec = F.log_softmax(self.linear(bow_vec))\n",
    "        bow_vec = self.layer(bow_vec)\n",
    "        return bow_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoWClassifier (\n",
      "  (layer): Sequential (\n",
      "    (0): Linear (30 -> 2)\n",
      "    (1): LogSoftmax ()\n",
      "  )\n",
      ")\n",
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.0302 -0.0156  0.0214 -0.0253 -0.1313  0.1603 -0.1102  0.1017  0.1098  0.0789\n",
      "-0.0288  0.0627  0.1672 -0.0322  0.0121 -0.1104  0.0701 -0.0768 -0.0674 -0.1307\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1710  0.1106 -0.0681 -0.1487  0.0702  0.0066  0.1374  0.1333  0.1441  0.1202\n",
      " 0.0681  0.1035  0.1222 -0.0319 -0.1759 -0.1701  0.0913  0.0453  0.1785  0.0587\n",
      "\n",
      "Columns 20 to 29 \n",
      "-0.1515  0.1204 -0.1683 -0.0829 -0.1206 -0.1609  0.1381  0.0623 -0.1467  0.0340\n",
      " 0.0906 -0.0736 -0.0802 -0.0197  0.1056 -0.1015 -0.1449 -0.1558 -0.0190 -0.0112\n",
      "[torch.FloatTensor of size 2x30]\n",
      "\n",
      "Parameter containing:\n",
      " 0.1492\n",
      "-0.1475\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "print(model)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'>\n",
      "Variable containing:\n",
      "-0.7263 -0.6610\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "print(type(bow_vector))\n",
    "log_probs = model(Variable(bow_vector))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_to_ix = { \"CHINESE\": 0, \"ENGLISH\": 1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.2965 -1.3604\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.3982 -1.1134\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for instance, label in test_data:\n",
    "    bow_vec = Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.0302 -0.0156  0.0214 -0.0253 -0.1313  0.1603 -0.1102  0.1017  0.1098  0.0789\n",
      "-0.0288  0.0627  0.1672 -0.0322  0.0121 -0.1104  0.0701 -0.0768 -0.0674 -0.1307\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1710  0.1106 -0.0681 -0.1487  0.0702  0.0066  0.1374  0.1333  0.1441  0.1202\n",
      " 0.0681  0.1035  0.1222 -0.0319 -0.1759 -0.1701  0.0913  0.0453  0.1785  0.0587\n",
      "\n",
      "Columns 20 to 29 \n",
      "-0.1515  0.1204 -0.1683 -0.0829 -0.1206 -0.1609  0.1381  0.0623 -0.1467  0.0340\n",
      " 0.0906 -0.0736 -0.0802 -0.0197  0.1056 -0.1015 -0.1449 -0.1558 -0.0190 -0.0112\n",
      "[torch.FloatTensor of size 2x30]\n",
      "\n",
      "Variable containing:\n",
      " 0.1106\n",
      " 0.1035\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()))\n",
    "print(next(model.parameters())[:, word_to_ix[\"喜欢\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/100: Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.5788\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 20/100: Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.1852\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 40/100: Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8355\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 60/100: Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.5226\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 80/100: Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.2411\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        bow_vec =  Variable(make_bow_vector(instance, word_to_ix))\n",
    "        target = Variable(make_target(label, label_to_ix))\n",
    "        \n",
    "        log_probs = model(bow_vec)\n",
    "        \n",
    "        loss = loss_function(log_probs, target)\n",
    "    if not epoch % 20:\n",
    "        print(\"epoch {}/100: {}\".format(epoch, loss))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1799 -1.8038\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.2464 -1.5215\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.0302 -0.0156  0.0214 -0.0253 -0.1313  0.1603 -0.1102  0.1017  0.3898  0.0789\n",
      "-0.0288  0.0627  0.1672 -0.0322  0.0121 -0.1104  0.0701 -0.0768 -0.3473 -0.1307\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1710  0.3905  0.2118  0.1313  0.0702  0.0066  0.1374  0.1333  0.1441  0.1202\n",
      " 0.0681 -0.1765 -0.1578 -0.3119 -0.1759 -0.1701  0.0913  0.0453  0.1785  0.0587\n",
      "\n",
      "Columns 20 to 29 \n",
      "-0.1515  0.1204 -0.1683 -0.0829 -0.1206 -0.1609  0.1381  0.0623 -0.1467  0.0340\n",
      " 0.0906 -0.0736 -0.0802 -0.0197  0.1056 -0.1015 -0.1449 -0.1558 -0.0190 -0.0112\n",
      "[torch.FloatTensor of size 2x30]\n",
      "\n",
      "Variable containing:\n",
      " 0.3905\n",
      "-0.1765\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for instance, label in test_data:\n",
    "    bow_vec = Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "    \n",
    "print(next(model.parameters()))\n",
    "print(next(model.parameters())[:, word_to_ix[\"喜欢\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.0565 -0.9923 -0.3660  0.6203  0.7167  0.5366\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       "[torch.LongTensor of size 1]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word embeddings\n",
    "ix_to_word = dict(enumerate(list(jieba.cut(\"我喜欢你\"))))\n",
    "word_to_ix = dict(zip(ix_to_word.values(), ix_to_word.keys()))\n",
    "embeds = nn.Embedding(3, 6)\n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"喜欢\"]])\n",
    "print(embeds(Variable(lookup_tensor)))\n",
    "lookup_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### N-gram Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n",
      "['前天', '久违', '的', '大概', '隔', '了', '一个月', '把', '自己', '割', '了', '由于', '忍', '了', '太久', '的', '缘故', '下手', '不知轻重', '割', '得', '有些', '深血', '不是', '一点一点', '渗在', '皮肤', '表面', '上', '的', '而是', '直接', '大颗', '大颗', '的', '涌出来', '给', '我', '的', '感觉', '是', '凉凉的', '很多', '顺着', '我', '的', '手腕', '汇成', '一股', '沿着', '我', '的', '手掌', '地', '在', '地上', '我', '的', '拖鞋', '上', '哪里', '都', '是', '血', '浣熊', '在', '敲门', '又', '干嚎', '我', '不', '记得', '了', '总之', '很', '混乱', '我', '不得不', '走', '出去', '然后', '他', '吓坏', '了', '一直', '哭', '声音', '很大', '隔壁', '人家', '以为', '出', '了', '事来', '敲', '我们', '的', '门', '血滴', '了', '很多', '在', '房间', '和', '餐厅', '的', '地板', '上', '我', '拿', '一些', '纸巾', '摁', '在', '手腕', '上', '止血', '但', '没什么', '用', '我', '换', '了', '很多', '张', '纸巾', '浣熊', '一直', '蹲', '在', '地上', '哭天', '哪天', '哪地', '叫', '着', '我', '哭', '不', '出来', '我', '甚至', '一点', '表情', '都', '做', '不', '出来', '换句话说', '我', '那个', '时候', '很', '奇怪', '的', '麻木', '了', '我', '完全', '不', '知道', '先', '做', '什么', '要', '安抚', '浣熊', '还是', '给', '自己', '处理', '伤口', '我', '就', '呆坐', '在', '那里', '然后', '我', '小', '小声', '的', '说', '没事', '的']\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "raw_sen = \"\"\"前天久违的（大概隔了一个月）把自己割了。\n",
    "由于忍了太久的缘故，下手不知轻重，割得有些深，血不是一点一点渗在皮肤表面上的，\n",
    "而是直接大颗大颗的涌出来，给我的感觉是凉凉的，很多，顺着我的手腕汇成一股，\n",
    "沿着我的手掌地在地上，我的拖鞋上，哪里都是血。浣熊在敲门，又干嚎，我不记得了，\n",
    "总之很混乱，我不得不走出去。然后他吓坏了，一直哭，声音很大，\n",
    "隔壁人家以为出了事来敲我们的门。血滴了很多在房间和餐厅的地板上，\n",
    "我拿一些纸巾摁在手腕上止血，但没什么用。我换了很多张纸巾，\n",
    "浣熊一直蹲在地上哭，“天哪天哪”地叫着，我哭不出来，我甚至一点表情都做不出来，\n",
    "换句话说，我那个时候很奇怪的麻木了，我完全不知道先做什么，要安抚浣熊，\n",
    "还是给自己处理伤口，我就呆坐在那里。然后我小小声的说“没事的”。\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "raw_sen = re.sub(r'[!。（），”“\\n]', r'', raw_sen)\n",
    "raw_sen\n",
    "\n",
    "test_sentence = list(jieba.cut(raw_sen))\n",
    "print(len(test_sentence))\n",
    "print(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['前天', '久违'], '的'), (['久违', '的'], '大概'), (['的', '大概'], '隔')]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 3-gram\n",
    "trigrams = [([test_sentence[i], test_sentence[i+1]], test_sentence[i+2]) for i in range(len(test_sentence) - 2)]\n",
    "trigrams[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '甚至',\n",
       " 1: '割',\n",
       " 2: '声音',\n",
       " 3: '血滴',\n",
       " 4: '没什么',\n",
       " 5: '什么',\n",
       " 6: '给',\n",
       " 7: '不',\n",
       " 8: '那里',\n",
       " 9: '小',\n",
       " 10: '人家',\n",
       " 11: '在',\n",
       " 12: '皮肤',\n",
       " 13: '然后',\n",
       " 14: '汇成',\n",
       " 15: '哪地',\n",
       " 16: '事来',\n",
       " 17: '就',\n",
       " 18: '都',\n",
       " 19: '摁',\n",
       " 20: '下手',\n",
       " 21: '以为',\n",
       " 22: '一点一点',\n",
       " 23: '哪里',\n",
       " 24: '还是',\n",
       " 25: '做',\n",
       " 26: '而是',\n",
       " 27: '呆坐',\n",
       " 28: '自己',\n",
       " 29: '出来',\n",
       " 30: '大颗',\n",
       " 31: '出去',\n",
       " 32: '着',\n",
       " 33: '把',\n",
       " 34: '地',\n",
       " 35: '隔',\n",
       " 36: '用',\n",
       " 37: '叫',\n",
       " 38: '纸巾',\n",
       " 39: '拿',\n",
       " 40: '血',\n",
       " 41: '张',\n",
       " 42: '说',\n",
       " 43: '一个月',\n",
       " 44: '记得',\n",
       " 45: '安抚',\n",
       " 46: '止血',\n",
       " 47: '上',\n",
       " 48: '太久',\n",
       " 49: '他',\n",
       " 50: '敲',\n",
       " 51: '哪天',\n",
       " 52: '换句话说',\n",
       " 53: '不知轻重',\n",
       " 54: '地上',\n",
       " 55: '很',\n",
       " 56: '地板',\n",
       " 57: '感觉',\n",
       " 58: '凉凉的',\n",
       " 59: '换',\n",
       " 60: '不是',\n",
       " 61: '一点',\n",
       " 62: '处理',\n",
       " 63: '很多',\n",
       " 64: '沿着',\n",
       " 65: '渗在',\n",
       " 66: '缘故',\n",
       " 67: '很大',\n",
       " 68: '门',\n",
       " 69: '我们',\n",
       " 70: '得',\n",
       " 71: '拖鞋',\n",
       " 72: '没事',\n",
       " 73: '那个',\n",
       " 74: '有些',\n",
       " 75: '深血',\n",
       " 76: '要',\n",
       " 77: '出',\n",
       " 78: '但',\n",
       " 79: '手腕',\n",
       " 80: '顺着',\n",
       " 81: '我',\n",
       " 82: '手掌',\n",
       " 83: '混乱',\n",
       " 84: '大概',\n",
       " 85: '小声',\n",
       " 86: '麻木',\n",
       " 87: '蹲',\n",
       " 88: '完全',\n",
       " 89: '哭',\n",
       " 90: '走',\n",
       " 91: '时候',\n",
       " 92: '伤口',\n",
       " 93: '一直',\n",
       " 94: '表面',\n",
       " 95: '的',\n",
       " 96: '前天',\n",
       " 97: '敲门',\n",
       " 98: '总之',\n",
       " 99: '房间',\n",
       " 100: '隔壁',\n",
       " 101: '干嚎',\n",
       " 102: '不得不',\n",
       " 103: '餐厅',\n",
       " 104: '了',\n",
       " 105: '又',\n",
       " 106: '吓坏',\n",
       " 107: '和',\n",
       " 108: '直接',\n",
       " 109: '一些',\n",
       " 110: '奇怪',\n",
       " 111: '是',\n",
       " 112: '表情',\n",
       " 113: '久违',\n",
       " 114: '由于',\n",
       " 115: '知道',\n",
       " 116: '浣熊',\n",
       " 117: '先',\n",
       " 118: '涌出来',\n",
       " 119: '哭天',\n",
       " 120: '一股',\n",
       " 121: '忍'}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        embeds = self.embeddings(X).view(1, -1)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 Loss: \n",
      " 877.4662\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Epoch: 20/100 Loss: \n",
      " 765.2822\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Epoch: 40/100 Loss: \n",
      " 688.0721\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Epoch: 60/100 Loss: \n",
      " 612.6309\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Epoch: 80/100 Loss: \n",
      " 530.2733\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "#         print(context)\n",
    "        context_idxs = list(map(lambda w: word_to_ix[w], context))\n",
    "        context_var = Variable(\n",
    "            torch.LongTensor(context_idxs).view(-1, 1)\n",
    "        )\n",
    "#         print(context_var.size())\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_var)\n",
    "        \n",
    "#         loss = loss_function(log_probs, Variable(torch.LongTensor([word_to_ix[target]])))    \n",
    "        loss = loss_function(\n",
    "            log_probs, \n",
    "            Variable(torch.LongTensor([word_to_ix[target]]))\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data\n",
    "    if not epoch % 20:\n",
    "        print(\"Epoch: {}/100 Loss: {}\".format(epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = Variable(torch.LongTensor([[word_to_ix[\"我\"]], [word_to_ix[\"感觉\"]]]).view(-1, 1))\n",
    "ix_to_word[np.argmin(model(input_test).data.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_word[np.argmax(model(input_test).data.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.8453712"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model(input_test).data.numpy()\n",
    "a[0][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model(input_test).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['前', '天', '违', '的'], '久'), (['天', '久', '的', '大'], '违'), (['久', '违', '大', '概'], '的'), (['违', '的', '概', '隔'], '大'), (['的', '大', '隔', '了'], '概')]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(2, len(raw_sen) - 2):\n",
    "    context = [raw_sen[i-2], raw_sen[i-1], raw_sen[i+1], raw_sen[i+2]]\n",
    "    target = raw_sen[i]\n",
    "    data.append( (context, target) )\n",
    "\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CBOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function = nn.NLLLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=.001)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
